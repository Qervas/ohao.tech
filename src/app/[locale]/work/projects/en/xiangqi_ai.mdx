---
title: "Chinese Chess AI with Deep Q-Learning"
publishedAt: "2024-12-20"
updatedAt: "2024-12-20"
summary: "A CUDA-accelerated Chinese Chess (Xiangqi) AI using Deep Q-Networks for reinforcement learning. Features self-play training, multiple game modes, and real-time visualization of training progress. Demonstrates significant speedup using GPU acceleration for neural network computations."
status: "completed"
featured: true
images:
  - "/images/projects/xiangqi/board_AI.png"
  - "/images/projects/xiangqi/train.png"
  - "/images/projects/xiangqi/system.png"

team:
  - name: "Shaoxuan Yin"
    role: "AI Engineer"
    avatar: "/images/me_snap.png"
    linkedIn: "https://www.linkedin.com/in/shaoxuan-yin-021548170/"
---

## Github Repo

<div style={{ display: 'flex', gap: '1rem', marginBottom: '2rem' }}>
  <Button
    variant="primary"
    size="l"
    href="https://github.com/Qervas/cn_chess_ai"
    prefixIcon="github"
    label="Chinese Chess AI"
  />
</div>

## Project Gallery

<Carousel
  images={[
    {
      src: "/images/projects/xiangqi/board_AI.png",
      alt: "Interactive Chinese Chess board with AI opponent",
      width: 1920,  // optional, will be calculated from image if not provided
      height: 1080  // optional, will be calculated from image if not provided
    },
    {
      src: "/images/projects/xiangqi/train.png",
      alt: "Training visualization"
    },
    {
      src: "/images/projects/xiangqi/system.png",
      alt: "System architecture"
    }
  ]}
  aspectRatio="auto"
  indicator="thumbnail"
  sizes="(max-width: 768px) 100vw, (max-width: 1200px) 75vw, 66vw"
/>

## Overview

An implementation of Chinese Chess (Xiangqi) AI using Deep Q-Learning Networks (DQN) with CUDA acceleration. The project features:

<Grid columns="repeat(2, 1fr)" gap="16">
  <Flex direction="column" background="surface" padding="24" radius="l" border="neutral-medium" borderStyle="solid-1">
    <Heading variant="heading-strong-m">Core Features</Heading>
    <ul>
      <li>Deep Q-Network for reinforcement learning</li>
      <li>CUDA-accelerated neural network training</li>
      <li>Self-play training mechanism</li>
      <li>Multiple game modes (Human vs Human/AI, AI vs AI)</li>
      <li>Real-time training visualization</li>
    </ul>
  </Flex>

  <Flex direction="column" background="surface" padding="24" radius="l" border="neutral-medium" borderStyle="solid-1">
    <Heading variant="heading-strong-m">Technical Highlights</Heading>
    <ul>
      <li>Experience replay buffer for stable learning</li>
      <li>Target network for Q-learning stability</li>
      <li>GPU-accelerated matrix operations</li>
      <li>Efficient state representation</li>
      <li>Progressive win-rate tracking</li>
    </ul>
  </Flex>
</Grid>

## Technical Documentation üìë

<Flex direction="column" gap="16" background="surface" padding="24" radius="l" border="neutral-medium" borderStyle="solid-1">
  <Flex gap="12" alignItems="center">
    <Icon name="document" size="xl" onBackground="brand-strong"/>
    <Text variant="heading-strong-l">
      Technical Paper
    </Text>
  </Flex>

  <Flex
    background="brand-weak"
    padding="24"
    radius="m"
    direction="column"
    gap="16"
    style={{
      boxShadow: 'var(--shadow-m)',
    }}
  >
    <Flex direction="column" gap="12">
      <Text variant="heading-strong-m">
        Chinese Chess AI: Deep Q-Learning Implementation
      </Text>

      <Flex gap="8" alignItems="center">
        <Icon name="calendar" size="s" onBackground="neutral-medium"/>
        <Text variant="body-default-s" onBackground="neutral-medium">
          Jan 15th 2024
        </Text>
        <Icon name="document" size="s" onBackground="neutral-medium"/>
        <Text variant="body-default-s" onBackground="neutral-medium">
          7 pages
        </Text>
      </Flex>
    </Flex>

    <Button
      variant="primary"
      size="l"
      href="/pdf/xiangqi_ai.pdf"
      prefixIcon="document"
      label="View Technical Paper"
      style={{
        animation: 'pulse 2s infinite',
      }}
    />

    <div id="technical-pdf" style={{ marginTop: 'var(--static-space-24)' }}>
      <PDF url="/pdf/xiangqi_ai.pdf" width="100%" height="600px" />
    </div>
  </Flex>

  <Flex
    gap="8"
    marginTop="8"
    padding="12"
    radius="m"
    background="info-weak"
    alignItems="center"
    justifyContent="center"
  >
    <Icon name="info" size="s" onBackground="info-strong"/>
    <Text variant="body-default-s" onBackground="info-strong">
      This paper details the implementation of DQN, training methodology, and performance analysis
    </Text>
  </Flex>
</Flex>

<style>
{`
  @keyframes pulse {
    0% {
      transform: scale(1);
    }
    50% {
      transform: scale(1.02);
    }
    100% {
      transform: scale(1);
    }
  }
`}
</style>

## Implementation Details

<Grid columns="1" gap="24">
  <Flex direction="column" background="surface" padding="32" radius="l" border="neutral-medium" borderStyle="solid-1">
    <Heading variant="heading-strong-l" marginBottom="24">DQN Architecture</Heading>
    <CodeBlock
      codeInstances={[
        {
          language: "mermaid",
          code: `graph TB
        classDef default fill:secondaryColor,stroke:secondaryBorderColor,color:secondaryTextColor
        classDef highlight fill:primaryColor,stroke:primaryBorderColor,color:primaryTextColor

        A[Game State Input] --> B[Neural Network]
        B --> C[Q-Values Output]

        B --> D[Hidden Layer 128 nodes]
        D --> E[ReLU Activation]
        E --> F[Output Layer 8100 nodes]

        G[Experience Replay] --> B
        H[Target Network] --> B

        class A,B,C highlight`
        }
      ]}
      style={{ minHeight: "600px" }}
    />
  </Flex>
</Grid>

<Grid columns="1" gap="24">
  <Flex direction="column" background="surface" padding="32" radius="l" border="neutral-medium" borderStyle="solid-1">
    <Heading variant="heading-strong-l" marginBottom="24">Training Pipeline</Heading>
    <CodeBlock
      codeInstances={[
        {
          language: "mermaid",
          code: `graph TB
        classDef default fill:secondaryColor,stroke:secondaryBorderColor,color:secondaryTextColor
        classDef highlight fill:primaryColor,stroke:primaryBorderColor,color:primaryTextColor

        A[Game State] --> B[State Encoding]
        B --> C[DQN Forward Pass]
        C --> D[Action Selection]
        D --> E[Move Execution]
        E --> F[Reward Calculation]
        F --> G[Experience Storage]
        G --> H[Network Update]

        class A,D,H highlight`
        }
      ]}
      style={{ minHeight: "600px" }}
    />
  </Flex>
</Grid>

## Performance Analysis

<Grid columns="repeat(2, 1fr)" gap="16">
  <Flex direction="column" background="surface" padding="24" radius="l" border="neutral-medium" borderStyle="solid-1">
    <Heading variant="heading-strong-m">Training Progress</Heading>
    <CodeBlock
      codeInstances={[
        {
          language: "mermaid",
          label: "Win Rate vs Random Player",
          code: `pie
    title Win Rate After 1000 Games
    "Wins" : 65
    "Losses" : 35`
        }
      ]}
    />
    <Text variant="body-default-s" marginTop="8" onBackground="neutral-weak">
      AI achieves ~65% win rate against random opponent after 1000 games
    </Text>
  </Flex>
</Grid>

## Future Improvements

- Implement Monte Carlo Tree Search for move selection
- Add multi-agent training with diverse strategies
- Optimize network architecture for better convergence
- Enhance reward system with positional evaluation
- Implement distributed training across multiple GPUs

<Comment
  author="Note"
  content={
    <>
      This project demonstrates the potential of deep reinforcement learning in board games, while highlighting areas for future optimization.
      Feel free to contribute or suggest improvements!{' '}
      <span role="img" aria-label="chess">‚ôüÔ∏è</span>
    </>
  }
/>
